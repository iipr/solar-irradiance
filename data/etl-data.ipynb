{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to download and process the GHI measurements from [1] into HDF5 files.\n",
    "\n",
    "```\n",
    "[1] Sengupta, M.; Andreas, A. (2010). Oahu Solar Measurement Grid (1-Year Archive):\n",
    "1-Second Solar Irradiance; Oahu, Hawaii (Data); NREL Report No. DA-5500-56506.\n",
    "http://dx.doi.org/10.5439/1052451\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "import os, requests, zipfile, glob, warnings, time\n",
    "\n",
    "from pvlib.location import Location\n",
    "from pvlib.irradiance import clearsky_index\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, unzip=False, out_fold='../data/raw/'):\n",
    "    for yyyymm in ['2010{:0>2}'.format(mm) for mm in range(3, 13)] + \\\n",
    "                  ['2011{:0>2}'.format(mm) for mm in range(1, 11)]:\n",
    "        url_curr = url.format(yyyymm)\n",
    "        fname = os.path.basename(url_curr)\n",
    "        file_curr = requests.get(url_curr)\n",
    "        if file_curr.status_code == 200:\n",
    "            with open(out_fold + fname, 'wb') as to_file:\n",
    "                to_file.write(file_curr.content)\n",
    "            os.mkdir(out_fold + yyyymm)\n",
    "            if unzip:\n",
    "                with zipfile.ZipFile(out_fold + fname, 'r') as zipped:\n",
    "                    zipped.extractall(out_fold + yyyymm)\n",
    "            print(fname + ' downloaded{}.'.format(' and unzipped' if unzip else ''))\n",
    "        else:\n",
    "            print('Failed to download {}, status code is {}'.format(url_curr, file_curr.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201003.zip downloaded and unzipped.\n",
      "201004.zip downloaded and unzipped.\n",
      "201005.zip downloaded and unzipped.\n",
      "201006.zip downloaded and unzipped.\n",
      "201007.zip downloaded and unzipped.\n",
      "201008.zip downloaded and unzipped.\n",
      "201009.zip downloaded and unzipped.\n",
      "201010.zip downloaded and unzipped.\n",
      "201011.zip downloaded and unzipped.\n",
      "201012.zip downloaded and unzipped.\n",
      "201101.zip downloaded and unzipped.\n",
      "201102.zip downloaded and unzipped.\n",
      "201103.zip downloaded and unzipped.\n",
      "201104.zip downloaded and unzipped.\n",
      "201105.zip downloaded and unzipped.\n",
      "201106.zip downloaded and unzipped.\n",
      "201107.zip downloaded and unzipped.\n",
      "201108.zip downloaded and unzipped.\n",
      "201109.zip downloaded and unzipped.\n",
      "201110.zip downloaded and unzipped.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://midcdmz.nrel.gov/oahu_archive/rawdata/Oahu_GHI/{}.zip'\n",
    "get_data(url, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location of the 17 (+1) sensors:\n",
    "\n",
    "<img src=\"https://midcdmz.nrel.gov/oahu_archive/map.jpg\" alt=\"M1 - Table field\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata and line check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from https://midcdmz.nrel.gov/apps/html.pl?site=oahugrid;page=instruments#RSR:\n",
    "  - Data Sample rate for Global Horizontal and Tilt is every 1 second with 1 second outputs (data only collected from 5:00 to 20:00 HST).\n",
    "  - Data Sample rate for RSR Global Horizontal, Direct Normal and Diffuse Horizontal is every 3 seconds with 3 second outputs (data only collected from 5:00 to 20:00 HST).\n",
    "  - The CR800 is a table based operating system, data tables of equal output rate are merged together and converted to array (\"classic\") Campbell format after data collection, also any non-numeric values are converted to -99999. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like data for all dates starts being recorded at 5am and goes up to 8pm.  \n",
    "Thus, there should be: 15h * 60m * 60s + 1s = 54001 lines in each file, lets check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlines = 54001\n",
    "\n",
    "for folder in glob.glob('../data/raw/*'):\n",
    "    if os.path.isdir(folder):\n",
    "        for fname in glob.glob(folder + '/*'):\n",
    "            with open(fname) as file:\n",
    "                curr_lines = sum(1 for line in open(fname))\n",
    "                if curr_lines != nlines:\n",
    "                    print('{} has {} lines.'.format(fname, curr_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final file will look like this:\n",
    "```\n",
    "raw_carray.h5 (file)\n",
    "|-- years (group)\n",
    "    |-- months (group)\n",
    "        |-- days (group)\n",
    "            |-- sensors: time + radiation (carray)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Field # |\tInstrument | Units     |\n",
    "|---------|------------|-----------|\n",
    "| 1       |  Seconds   |  ss       |\n",
    "| 2       |  Year      |  -        |\n",
    "| 3       |  DOY       |  -        |\n",
    "| 4       |  HST       |  hhmm     |\n",
    "| 5       |  DH3       |  W/m$^2$  |\n",
    "| 6       |  DH4       |  W/m$^2$  |\n",
    "| 7       |  DH5       |  W/m$^2$  |\n",
    "| 8       |  DH10      |  W/m$^2$  |\n",
    "| 9       |  DH11      |  W/m$^2$  |\n",
    "| 10      |  DH9       |  W/m$^2$  |\n",
    "| 11      |  DH2       |  W/m$^2$  |\n",
    "| 12      |  DH1       |  W/m$^2$  |\n",
    "| 13      |  Tilt DH1  |  W/m$^2$  |\n",
    "| 14      |  AP6       |  W/m$^2$  |\n",
    "| 15      |  Tilt AP6  |  W/m$^2$  |\n",
    "| 16      |  AP1       |  W/m$^2$  |\n",
    "| 17      |  AP3       |  W/m$^2$  |\n",
    "| 18      |  AP5       |  W/m$^2$  |\n",
    "| 19      |  AP4       |  W/m$^2$  |\n",
    "| 20      |  AP7       |  W/m$^2$  |\n",
    "| 21      |  DH6       |  W/m$^2$  |\n",
    "| 22      |  DH7       |  W/m$^2$  |\n",
    "| 23      |  DH8       |  W/m$^2$  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Seconds', 'Year', 'DOY', 'HST', 'DH3', 'DH4', 'DH5', 'DH10', \n",
    "             'DH11', 'DH9', 'DH2', 'DH1', 'Tilt_DH1', 'AP6', 'Tilt_AP6', \n",
    "             'AP1', 'AP3', 'AP5', 'AP4', 'AP7', 'DH6', 'DH7', 'DH8', ]\n",
    "sensors = ['ap01', 'ap03', 'ap04', 'ap05', 'ap06', 'ap07', 'dh01', 'dh02', \n",
    "           'dh03', 'dh04', 'dh05', 'dh06', 'dh07', 'dh08', 'dh09', 'dh10', 'dh11']\n",
    "map_sensors = dict({\n",
    "    'ap01': 'AP1',\n",
    "    'ap03': 'AP3',\n",
    "    'ap04': 'AP4',\n",
    "    'ap05': 'AP5',\n",
    "    'ap06': 'AP6',\n",
    "    'ap07': 'AP7',\n",
    "    'dh01': 'DH1',\n",
    "    'dh02': 'DH2',\n",
    "    'dh03': 'DH3',\n",
    "    'dh04': 'DH4',\n",
    "    'dh05': 'DH5',\n",
    "    'dh06': 'DH6',\n",
    "    'dh07': 'DH7',\n",
    "    'dh08': 'DH8',\n",
    "    'dh09': 'DH9',\n",
    "    'dh10': 'DH10',\n",
    "    'dh11': 'DH11'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_raw(in_path, out_path, max_rows=54001, map_sensors=map_sensors):\n",
    "    # Parameters for the hdf5 file\n",
    "    filters = tb.Filters(complib='blosc:lz4hc', complevel=9)\n",
    "    dtype = tb.Float32Atom()\n",
    "    data_shape = (0, 2)\n",
    "    chunkshape=(max_rows, 2)\n",
    "    tic = time.time()\n",
    "    # Open file and transfer\n",
    "    with tb.open_file(out_path, mode='a', filters=filters) as hdf5_file, \\\n",
    "         warnings.catch_warnings() as w:\n",
    "        warnings.simplefilter('ignore', tb.NaturalNameWarning)\n",
    "        # For every month\n",
    "        for folder in glob.glob(in_path):\n",
    "            if os.path.isdir(folder):\n",
    "                year = int(os.path.basename(folder)[:4])\n",
    "                month = int(os.path.basename(folder)[4:])\n",
    "                # For every day inside that month\n",
    "                for fname in glob.glob(folder + '/*'):\n",
    "                    day = int(os.path.splitext(os.path.basename(fname))[0][-2:])\n",
    "                    df = pd.read_csv(fname, names=col_names)\n",
    "                    # Compute proper timestamp\n",
    "                    df.HST = df.HST.apply(lambda x: str(x).rjust(4, '0'))\n",
    "                    grp_name = '/{:04}/{:02}/{:02}'.format(year, month, day)\n",
    "                    df['ts'] = df.apply(lambda row: pd.Timestamp(year=year, month=month, \n",
    "                                                                 day=day, hour=int(row.HST[0:2]), \n",
    "                                                                 minute=int(row.HST[2:]), \n",
    "                                                                 second=row.Seconds),\n",
    "                                        axis=1)\n",
    "                    df['ts_epoch'] = df.ts.astype('int64')//1e9\n",
    "                    # Group name\n",
    "                    grp_name = '/{:04}/{:02}/{:02}'.format(year, month, day)\n",
    "                    # For every sensor\n",
    "                    for sensor in sensors:\n",
    "                        hdf5_file.create_array(obj=df[['ts_epoch', map_sensors[sensor]]].values, \n",
    "                                               where=grp_name, name=sensor,\n",
    "                                               createparents=True) #chunkshape=chunkshape,\n",
    "                    #return 'It took {} in total'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - tic)))\n",
    "    return 'It took {} in total'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:26:48 in total'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/raw/*'\n",
    "out_path = '../data/raw/raw_array.h5'\n",
    "transfer_raw(in_path, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference on how to open and check an `.h5` file:\n",
    "```\n",
    "hdf5_file = tb.open_file('../data/raw/test1.h5', mode='r')\n",
    "hdf5_file.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between sensors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://delicias.dia.fi.upm.es/nextcloud/index.php/s/fTFqB4Wx6PW8kgJ/preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sensor SN</th>\n",
       "      <th>CF [W/m^2/mV]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ap01</td>\n",
       "      <td>21.31276</td>\n",
       "      <td>-158.08389</td>\n",
       "      <td>PY66523</td>\n",
       "      <td>110.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ap03</td>\n",
       "      <td>21.31281</td>\n",
       "      <td>-158.08163</td>\n",
       "      <td>PY66524</td>\n",
       "      <td>113.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ap04</td>\n",
       "      <td>21.31141</td>\n",
       "      <td>-158.07947</td>\n",
       "      <td>PY66526</td>\n",
       "      <td>107.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ap05</td>\n",
       "      <td>21.30983</td>\n",
       "      <td>-158.08249</td>\n",
       "      <td>PY66525</td>\n",
       "      <td>116.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ap06</td>\n",
       "      <td>21.30812</td>\n",
       "      <td>-158.07935</td>\n",
       "      <td>PY66521</td>\n",
       "      <td>117.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ap07</td>\n",
       "      <td>21.31478</td>\n",
       "      <td>-158.07785</td>\n",
       "      <td>PY66527</td>\n",
       "      <td>117.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dh01</td>\n",
       "      <td>21.31533</td>\n",
       "      <td>-158.08700</td>\n",
       "      <td>PY66519</td>\n",
       "      <td>107.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dh02</td>\n",
       "      <td>21.31451</td>\n",
       "      <td>-158.08534</td>\n",
       "      <td>PY66505</td>\n",
       "      <td>106.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dh03</td>\n",
       "      <td>21.31236</td>\n",
       "      <td>-158.08463</td>\n",
       "      <td>PY66499</td>\n",
       "      <td>141.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dh04</td>\n",
       "      <td>21.31303</td>\n",
       "      <td>-158.08505</td>\n",
       "      <td>PY66500</td>\n",
       "      <td>118.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dh05</td>\n",
       "      <td>21.31357</td>\n",
       "      <td>-158.08424</td>\n",
       "      <td>PY66501</td>\n",
       "      <td>104.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dh06</td>\n",
       "      <td>21.31179</td>\n",
       "      <td>-158.08678</td>\n",
       "      <td>PY66528</td>\n",
       "      <td>112.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dh07</td>\n",
       "      <td>21.31418</td>\n",
       "      <td>-158.08685</td>\n",
       "      <td>PY66529</td>\n",
       "      <td>114.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dh08</td>\n",
       "      <td>21.31034</td>\n",
       "      <td>-158.08675</td>\n",
       "      <td>PY66530</td>\n",
       "      <td>117.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dh09</td>\n",
       "      <td>21.31268</td>\n",
       "      <td>-158.08688</td>\n",
       "      <td>PY66504</td>\n",
       "      <td>141.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dh10</td>\n",
       "      <td>21.31183</td>\n",
       "      <td>-158.08554</td>\n",
       "      <td>PY66502</td>\n",
       "      <td>109.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dh11</td>\n",
       "      <td>21.31042</td>\n",
       "      <td>-158.08530</td>\n",
       "      <td>PY66503</td>\n",
       "      <td>114.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Station  Latitude  Longitude Sensor SN  CF [W/m^2/mV]\n",
       "0     ap01  21.31276 -158.08389   PY66523         110.40\n",
       "1     ap03  21.31281 -158.08163   PY66524         113.55\n",
       "2     ap04  21.31141 -158.07947   PY66526         107.58\n",
       "3     ap05  21.30983 -158.08249   PY66525         116.04\n",
       "4     ap06  21.30812 -158.07935   PY66521         117.34\n",
       "5     ap07  21.31478 -158.07785   PY66527         117.61\n",
       "6     dh01  21.31533 -158.08700   PY66519         107.34\n",
       "7     dh02  21.31451 -158.08534   PY66505         106.39\n",
       "8     dh03  21.31236 -158.08463   PY66499         141.76\n",
       "9     dh04  21.31303 -158.08505   PY66500         118.01\n",
       "10    dh05  21.31357 -158.08424   PY66501         104.46\n",
       "11    dh06  21.31179 -158.08678   PY66528         112.83\n",
       "12    dh07  21.31418 -158.08685   PY66529         114.04\n",
       "13    dh08  21.31034 -158.08675   PY66530         117.75\n",
       "14    dh09  21.31268 -158.08688   PY66504         141.92\n",
       "15    dh10  21.31183 -158.08554   PY66502         109.06\n",
       "16    dh11  21.31042 -158.08530   PY66503         114.71"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st = pd.read_csv('./stations.txt', sep='\\t')\n",
    "df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.set_index('Station', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ap01</th>\n",
       "      <th>ap03</th>\n",
       "      <th>ap04</th>\n",
       "      <th>ap05</th>\n",
       "      <th>ap06</th>\n",
       "      <th>ap07</th>\n",
       "      <th>dh01</th>\n",
       "      <th>dh02</th>\n",
       "      <th>dh03</th>\n",
       "      <th>dh04</th>\n",
       "      <th>dh05</th>\n",
       "      <th>dh06</th>\n",
       "      <th>dh07</th>\n",
       "      <th>dh08</th>\n",
       "      <th>dh09</th>\n",
       "      <th>dh10</th>\n",
       "      <th>dh11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ap01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00226055</td>\n",
       "      <td>0.00462157</td>\n",
       "      <td>0.00324729</td>\n",
       "      <td>0.00649163</td>\n",
       "      <td>0.00636883</td>\n",
       "      <td>0.00403448</td>\n",
       "      <td>0.00227266</td>\n",
       "      <td>0.00084119</td>\n",
       "      <td>0.00119101</td>\n",
       "      <td>0.000882383</td>\n",
       "      <td>0.00304844</td>\n",
       "      <td>0.00328299</td>\n",
       "      <td>0.00374647</td>\n",
       "      <td>0.00299107</td>\n",
       "      <td>0.00189404</td>\n",
       "      <td>0.00273198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap03</th>\n",
       "      <td>0.00226055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00257402</td>\n",
       "      <td>0.00310161</td>\n",
       "      <td>0.00521483</td>\n",
       "      <td>0.00426255</td>\n",
       "      <td>0.00593189</td>\n",
       "      <td>0.00408094</td>\n",
       "      <td>0.00303356</td>\n",
       "      <td>0.00342707</td>\n",
       "      <td>0.0027184</td>\n",
       "      <td>0.00525004</td>\n",
       "      <td>0.00539679</td>\n",
       "      <td>0.00568465</td>\n",
       "      <td>0.00525161</td>\n",
       "      <td>0.00403094</td>\n",
       "      <td>0.00437961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap04</th>\n",
       "      <td>0.00462157</td>\n",
       "      <td>0.00257402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00340834</td>\n",
       "      <td>0.00329219</td>\n",
       "      <td>0.00373916</td>\n",
       "      <td>0.00848925</td>\n",
       "      <td>0.00663829</td>\n",
       "      <td>0.00524672</td>\n",
       "      <td>0.0058104</td>\n",
       "      <td>0.00523627</td>\n",
       "      <td>0.00731987</td>\n",
       "      <td>0.00788272</td>\n",
       "      <td>0.00735821</td>\n",
       "      <td>0.00751804</td>\n",
       "      <td>0.00608451</td>\n",
       "      <td>0.00591346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap05</th>\n",
       "      <td>0.00324729</td>\n",
       "      <td>0.00310161</td>\n",
       "      <td>0.00340834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00357543</td>\n",
       "      <td>0.0067847</td>\n",
       "      <td>0.00711267</td>\n",
       "      <td>0.0054795</td>\n",
       "      <td>0.00331368</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.00412918</td>\n",
       "      <td>0.00471653</td>\n",
       "      <td>0.0061589</td>\n",
       "      <td>0.00429042</td>\n",
       "      <td>0.00523399</td>\n",
       "      <td>0.00364726</td>\n",
       "      <td>0.00287127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap06</th>\n",
       "      <td>0.00649163</td>\n",
       "      <td>0.00521483</td>\n",
       "      <td>0.00329219</td>\n",
       "      <td>0.00357543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00682683</td>\n",
       "      <td>0.0105122</td>\n",
       "      <td>0.00875855</td>\n",
       "      <td>0.00677171</td>\n",
       "      <td>0.00752317</td>\n",
       "      <td>0.0073222</td>\n",
       "      <td>0.00828697</td>\n",
       "      <td>0.00964228</td>\n",
       "      <td>0.00772583</td>\n",
       "      <td>0.0088031</td>\n",
       "      <td>0.00721666</td>\n",
       "      <td>0.00637907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap07</th>\n",
       "      <td>0.00636883</td>\n",
       "      <td>0.00426255</td>\n",
       "      <td>0.00373916</td>\n",
       "      <td>0.0067847</td>\n",
       "      <td>0.00682683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00916652</td>\n",
       "      <td>0.00749486</td>\n",
       "      <td>0.00719894</td>\n",
       "      <td>0.00740962</td>\n",
       "      <td>0.00650355</td>\n",
       "      <td>0.00941727</td>\n",
       "      <td>0.00901998</td>\n",
       "      <td>0.00994603</td>\n",
       "      <td>0.00927097</td>\n",
       "      <td>0.00823642</td>\n",
       "      <td>0.00863204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh01</th>\n",
       "      <td>0.00403448</td>\n",
       "      <td>0.00593189</td>\n",
       "      <td>0.00848925</td>\n",
       "      <td>0.00711267</td>\n",
       "      <td>0.0105122</td>\n",
       "      <td>0.00916652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00185149</td>\n",
       "      <td>0.00379971</td>\n",
       "      <td>0.00301538</td>\n",
       "      <td>0.00327341</td>\n",
       "      <td>0.00354683</td>\n",
       "      <td>0.00115974</td>\n",
       "      <td>0.00499626</td>\n",
       "      <td>0.00265272</td>\n",
       "      <td>0.00379231</td>\n",
       "      <td>0.00519597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh02</th>\n",
       "      <td>0.00227266</td>\n",
       "      <td>0.00408094</td>\n",
       "      <td>0.00663829</td>\n",
       "      <td>0.0054795</td>\n",
       "      <td>0.00875855</td>\n",
       "      <td>0.00749486</td>\n",
       "      <td>0.00185149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0022642</td>\n",
       "      <td>0.00150814</td>\n",
       "      <td>0.00144693</td>\n",
       "      <td>0.00307766</td>\n",
       "      <td>0.00154564</td>\n",
       "      <td>0.00440193</td>\n",
       "      <td>0.00239176</td>\n",
       "      <td>0.00268745</td>\n",
       "      <td>0.0040902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh03</th>\n",
       "      <td>0.00084119</td>\n",
       "      <td>0.00303356</td>\n",
       "      <td>0.00524672</td>\n",
       "      <td>0.00331368</td>\n",
       "      <td>0.00677171</td>\n",
       "      <td>0.00719894</td>\n",
       "      <td>0.00379971</td>\n",
       "      <td>0.0022642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000790759</td>\n",
       "      <td>0.0012713</td>\n",
       "      <td>0.00222428</td>\n",
       "      <td>0.00287068</td>\n",
       "      <td>0.00292828</td>\n",
       "      <td>0.00227264</td>\n",
       "      <td>0.00105309</td>\n",
       "      <td>0.00205244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh04</th>\n",
       "      <td>0.00119101</td>\n",
       "      <td>0.00342707</td>\n",
       "      <td>0.0058104</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.00752317</td>\n",
       "      <td>0.00740962</td>\n",
       "      <td>0.00301538</td>\n",
       "      <td>0.00150814</td>\n",
       "      <td>0.000790759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000973499</td>\n",
       "      <td>0.0021285</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.00318215</td>\n",
       "      <td>0.00186317</td>\n",
       "      <td>0.00129619</td>\n",
       "      <td>0.00262195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh05</th>\n",
       "      <td>0.000882383</td>\n",
       "      <td>0.0027184</td>\n",
       "      <td>0.00523627</td>\n",
       "      <td>0.00412918</td>\n",
       "      <td>0.0073222</td>\n",
       "      <td>0.00650355</td>\n",
       "      <td>0.00327341</td>\n",
       "      <td>0.00144693</td>\n",
       "      <td>0.0012713</td>\n",
       "      <td>0.000973499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00310161</td>\n",
       "      <td>0.00268034</td>\n",
       "      <td>0.0040906</td>\n",
       "      <td>0.00278598</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.00332357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh06</th>\n",
       "      <td>0.00304844</td>\n",
       "      <td>0.00525004</td>\n",
       "      <td>0.00731987</td>\n",
       "      <td>0.00471653</td>\n",
       "      <td>0.00828697</td>\n",
       "      <td>0.00941727</td>\n",
       "      <td>0.00354683</td>\n",
       "      <td>0.00307766</td>\n",
       "      <td>0.00222428</td>\n",
       "      <td>0.0021285</td>\n",
       "      <td>0.00310161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00239102</td>\n",
       "      <td>0.00145031</td>\n",
       "      <td>0.0008956</td>\n",
       "      <td>0.00124064</td>\n",
       "      <td>0.00201675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh07</th>\n",
       "      <td>0.00328299</td>\n",
       "      <td>0.00539679</td>\n",
       "      <td>0.00788272</td>\n",
       "      <td>0.0061589</td>\n",
       "      <td>0.00964228</td>\n",
       "      <td>0.00901998</td>\n",
       "      <td>0.00115974</td>\n",
       "      <td>0.00154564</td>\n",
       "      <td>0.00287068</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.00268034</td>\n",
       "      <td>0.00239102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0038413</td>\n",
       "      <td>0.0015003</td>\n",
       "      <td>0.00269046</td>\n",
       "      <td>0.00406695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh08</th>\n",
       "      <td>0.00374647</td>\n",
       "      <td>0.00568465</td>\n",
       "      <td>0.00735821</td>\n",
       "      <td>0.00429042</td>\n",
       "      <td>0.00772583</td>\n",
       "      <td>0.00994603</td>\n",
       "      <td>0.00499626</td>\n",
       "      <td>0.00440193</td>\n",
       "      <td>0.00292828</td>\n",
       "      <td>0.00318215</td>\n",
       "      <td>0.0040906</td>\n",
       "      <td>0.00145031</td>\n",
       "      <td>0.0038413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00234361</td>\n",
       "      <td>0.00191943</td>\n",
       "      <td>0.00145221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh09</th>\n",
       "      <td>0.00299107</td>\n",
       "      <td>0.00525161</td>\n",
       "      <td>0.00751804</td>\n",
       "      <td>0.00523399</td>\n",
       "      <td>0.0088031</td>\n",
       "      <td>0.00927097</td>\n",
       "      <td>0.00265272</td>\n",
       "      <td>0.00239176</td>\n",
       "      <td>0.00227264</td>\n",
       "      <td>0.00186317</td>\n",
       "      <td>0.00278598</td>\n",
       "      <td>0.0008956</td>\n",
       "      <td>0.0015003</td>\n",
       "      <td>0.00234361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00158685</td>\n",
       "      <td>0.00275754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh10</th>\n",
       "      <td>0.00189404</td>\n",
       "      <td>0.00403094</td>\n",
       "      <td>0.00608451</td>\n",
       "      <td>0.00364726</td>\n",
       "      <td>0.00721666</td>\n",
       "      <td>0.00823642</td>\n",
       "      <td>0.00379231</td>\n",
       "      <td>0.00268745</td>\n",
       "      <td>0.00105309</td>\n",
       "      <td>0.00129619</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.00124064</td>\n",
       "      <td>0.00269046</td>\n",
       "      <td>0.00191943</td>\n",
       "      <td>0.00158685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00143028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dh11</th>\n",
       "      <td>0.00273198</td>\n",
       "      <td>0.00437961</td>\n",
       "      <td>0.00591346</td>\n",
       "      <td>0.00287127</td>\n",
       "      <td>0.00637907</td>\n",
       "      <td>0.00863204</td>\n",
       "      <td>0.00519597</td>\n",
       "      <td>0.0040902</td>\n",
       "      <td>0.00205244</td>\n",
       "      <td>0.00262195</td>\n",
       "      <td>0.00332357</td>\n",
       "      <td>0.00201675</td>\n",
       "      <td>0.00406695</td>\n",
       "      <td>0.00145221</td>\n",
       "      <td>0.00275754</td>\n",
       "      <td>0.00143028</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ap01        ap03        ap04        ap05        ap06        ap07  \\\n",
       "ap01          NaN  0.00226055  0.00462157  0.00324729  0.00649163  0.00636883   \n",
       "ap03   0.00226055         NaN  0.00257402  0.00310161  0.00521483  0.00426255   \n",
       "ap04   0.00462157  0.00257402         NaN  0.00340834  0.00329219  0.00373916   \n",
       "ap05   0.00324729  0.00310161  0.00340834         NaN  0.00357543   0.0067847   \n",
       "ap06   0.00649163  0.00521483  0.00329219  0.00357543         NaN  0.00682683   \n",
       "ap07   0.00636883  0.00426255  0.00373916   0.0067847  0.00682683         NaN   \n",
       "dh01   0.00403448  0.00593189  0.00848925  0.00711267   0.0105122  0.00916652   \n",
       "dh02   0.00227266  0.00408094  0.00663829   0.0054795  0.00875855  0.00749486   \n",
       "dh03   0.00084119  0.00303356  0.00524672  0.00331368  0.00677171  0.00719894   \n",
       "dh04   0.00119101  0.00342707   0.0058104    0.004098  0.00752317  0.00740962   \n",
       "dh05  0.000882383   0.0027184  0.00523627  0.00412918   0.0073222  0.00650355   \n",
       "dh06   0.00304844  0.00525004  0.00731987  0.00471653  0.00828697  0.00941727   \n",
       "dh07   0.00328299  0.00539679  0.00788272   0.0061589  0.00964228  0.00901998   \n",
       "dh08   0.00374647  0.00568465  0.00735821  0.00429042  0.00772583  0.00994603   \n",
       "dh09   0.00299107  0.00525161  0.00751804  0.00523399   0.0088031  0.00927097   \n",
       "dh10   0.00189404  0.00403094  0.00608451  0.00364726  0.00721666  0.00823642   \n",
       "dh11   0.00273198  0.00437961  0.00591346  0.00287127  0.00637907  0.00863204   \n",
       "\n",
       "            dh01        dh02         dh03         dh04         dh05  \\\n",
       "ap01  0.00403448  0.00227266   0.00084119   0.00119101  0.000882383   \n",
       "ap03  0.00593189  0.00408094   0.00303356   0.00342707    0.0027184   \n",
       "ap04  0.00848925  0.00663829   0.00524672    0.0058104   0.00523627   \n",
       "ap05  0.00711267   0.0054795   0.00331368     0.004098   0.00412918   \n",
       "ap06   0.0105122  0.00875855   0.00677171   0.00752317    0.0073222   \n",
       "ap07  0.00916652  0.00749486   0.00719894   0.00740962   0.00650355   \n",
       "dh01         NaN  0.00185149   0.00379971   0.00301538   0.00327341   \n",
       "dh02  0.00185149         NaN    0.0022642   0.00150814   0.00144693   \n",
       "dh03  0.00379971   0.0022642          NaN  0.000790759    0.0012713   \n",
       "dh04  0.00301538  0.00150814  0.000790759          NaN  0.000973499   \n",
       "dh05  0.00327341  0.00144693    0.0012713  0.000973499          NaN   \n",
       "dh06  0.00354683  0.00307766   0.00222428    0.0021285   0.00310161   \n",
       "dh07  0.00115974  0.00154564   0.00287068     0.002136   0.00268034   \n",
       "dh08  0.00499626  0.00440193   0.00292828   0.00318215    0.0040906   \n",
       "dh09  0.00265272  0.00239176   0.00227264   0.00186317   0.00278598   \n",
       "dh10  0.00379231  0.00268745   0.00105309   0.00129619     0.002172   \n",
       "dh11  0.00519597   0.0040902   0.00205244   0.00262195   0.00332357   \n",
       "\n",
       "            dh06        dh07        dh08        dh09        dh10        dh11  \n",
       "ap01  0.00304844  0.00328299  0.00374647  0.00299107  0.00189404  0.00273198  \n",
       "ap03  0.00525004  0.00539679  0.00568465  0.00525161  0.00403094  0.00437961  \n",
       "ap04  0.00731987  0.00788272  0.00735821  0.00751804  0.00608451  0.00591346  \n",
       "ap05  0.00471653   0.0061589  0.00429042  0.00523399  0.00364726  0.00287127  \n",
       "ap06  0.00828697  0.00964228  0.00772583   0.0088031  0.00721666  0.00637907  \n",
       "ap07  0.00941727  0.00901998  0.00994603  0.00927097  0.00823642  0.00863204  \n",
       "dh01  0.00354683  0.00115974  0.00499626  0.00265272  0.00379231  0.00519597  \n",
       "dh02  0.00307766  0.00154564  0.00440193  0.00239176  0.00268745   0.0040902  \n",
       "dh03  0.00222428  0.00287068  0.00292828  0.00227264  0.00105309  0.00205244  \n",
       "dh04   0.0021285    0.002136  0.00318215  0.00186317  0.00129619  0.00262195  \n",
       "dh05  0.00310161  0.00268034   0.0040906  0.00278598    0.002172  0.00332357  \n",
       "dh06         NaN  0.00239102  0.00145031   0.0008956  0.00124064  0.00201675  \n",
       "dh07  0.00239102         NaN   0.0038413   0.0015003  0.00269046  0.00406695  \n",
       "dh08  0.00145031   0.0038413         NaN  0.00234361  0.00191943  0.00145221  \n",
       "dh09   0.0008956   0.0015003  0.00234361         NaN  0.00158685  0.00275754  \n",
       "dh10  0.00124064  0.00269046  0.00191943  0.00158685         NaN  0.00143028  \n",
       "dh11  0.00201675  0.00406695  0.00145221  0.00275754  0.00143028         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors = df_st.index.values\n",
    "n_sensors = len(sensors)\n",
    "\n",
    "df_dist = pd.DataFrame(np.empty((n_sensors, n_sensors)).fill(np.nan), \n",
    "                       index=sensors, columns=sensors)\n",
    "\n",
    "for s1 in sensors:\n",
    "    for s2 in sensors:\n",
    "        if s1 != s2:\n",
    "            dist = np.sqrt((df_st.loc[s1, 'Longitude'] - df_st.loc[s2, 'Longitude']) ** 2 +\\\n",
    "                           (df_st.loc[s1, 'Latitude'] - df_st.loc[s2, 'Latitude']) ** 2)\n",
    "            df_dist.loc[s1, s2] = dist\n",
    "df_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ap01</th>\n",
       "      <th>ap03</th>\n",
       "      <th>ap04</th>\n",
       "      <th>ap05</th>\n",
       "      <th>ap06</th>\n",
       "      <th>ap07</th>\n",
       "      <th>dh01</th>\n",
       "      <th>dh02</th>\n",
       "      <th>dh03</th>\n",
       "      <th>dh04</th>\n",
       "      <th>dh05</th>\n",
       "      <th>dh06</th>\n",
       "      <th>dh07</th>\n",
       "      <th>dh08</th>\n",
       "      <th>dh09</th>\n",
       "      <th>dh10</th>\n",
       "      <th>dh11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dh03</td>\n",
       "      <td>ap01</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh11</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh03</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dh05</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap03</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh04</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dh04</td>\n",
       "      <td>dh05</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap01</td>\n",
       "      <td>ap03</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh10</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh08</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dh10</td>\n",
       "      <td>dh03</td>\n",
       "      <td>ap07</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ap03</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap01</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap01</td>\n",
       "      <td>ap05</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dh02</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh05</td>\n",
       "      <td>ap06</td>\n",
       "      <td>dh03</td>\n",
       "      <td>ap06</td>\n",
       "      <td>dh06</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh08</td>\n",
       "      <td>ap01</td>\n",
       "      <td>ap01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dh11</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh10</td>\n",
       "      <td>ap07</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh06</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh10</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh08</td>\n",
       "      <td>dh09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dh09</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh09</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh03</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh05</td>\n",
       "      <td>ap05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dh06</td>\n",
       "      <td>ap07</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh02</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh02</td>\n",
       "      <td>ap01</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ap05</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh08</td>\n",
       "      <td>dh04</td>\n",
       "      <td>dh10</td>\n",
       "      <td>dh08</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh08</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh08</td>\n",
       "      <td>ap05</td>\n",
       "      <td>dh05</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dh07</td>\n",
       "      <td>ap06</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh08</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh11</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh08</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh11</td>\n",
       "      <td>dh02</td>\n",
       "      <td>ap01</td>\n",
       "      <td>ap05</td>\n",
       "      <td>dh02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dh08</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh06</td>\n",
       "      <td>dh07</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh08</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh08</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh01</td>\n",
       "      <td>ap05</td>\n",
       "      <td>dh01</td>\n",
       "      <td>ap03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dh01</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh08</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh02</td>\n",
       "      <td>dh01</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap05</td>\n",
       "      <td>dh01</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap03</td>\n",
       "      <td>ap05</td>\n",
       "      <td>ap03</td>\n",
       "      <td>ap03</td>\n",
       "      <td>ap03</td>\n",
       "      <td>dh01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ap04</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh09</td>\n",
       "      <td>dh09</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "      <td>ap04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ap07</td>\n",
       "      <td>dh08</td>\n",
       "      <td>dh07</td>\n",
       "      <td>ap07</td>\n",
       "      <td>dh07</td>\n",
       "      <td>dh06</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ap06</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh01</td>\n",
       "      <td>dh08</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap06</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap07</td>\n",
       "      <td>ap07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ap01  ap03  ap04  ap05  ap06  ap07  dh01  dh02  dh03  dh04  dh05  dh06  \\\n",
       "0   dh03  ap01  ap03  dh11  ap04  ap04  dh07  dh05  dh04  dh03  ap01  dh09   \n",
       "1   dh05  ap04  ap06  ap03  ap05  ap03  dh02  dh04  ap01  dh05  dh04  dh10   \n",
       "2   dh04  dh05  ap05  ap01  ap03  ap01  dh09  dh07  dh10  ap01  dh03  dh08   \n",
       "3   dh10  dh03  ap07  dh03  dh11  dh05  dh04  dh01  dh05  dh10  dh02  dh11   \n",
       "4   ap03  ap05  ap01  ap04  ap01  ap05  dh05  dh03  dh11  dh02  dh10  dh04   \n",
       "5   dh02  dh04  dh05  ap06  dh03  ap06  dh06  ap01  dh06  dh09  dh07  dh03   \n",
       "6   dh11  dh10  dh03  dh10  ap07  dh03  dh10  dh09  dh02  dh06  ap03  dh07   \n",
       "7   dh09  dh02  dh04  dh04  dh10  dh04  dh03  dh10  dh09  dh07  dh09  ap01   \n",
       "8   dh06  ap07  dh11  dh05  dh05  dh02  ap01  dh06  dh07  dh11  dh06  dh02   \n",
       "9   ap05  dh11  dh10  dh08  dh04  dh10  dh08  ap03  dh08  dh01  dh01  dh05   \n",
       "10  dh07  ap06  dh02  dh06  dh08  dh11  dh11  dh11  ap03  dh08  dh11  dh01   \n",
       "11  dh08  dh06  dh06  dh09  dh06  dh07  ap03  dh08  ap05  ap03  dh08  ap05   \n",
       "12  dh01  dh09  dh08  dh02  dh02  dh01  ap05  ap05  dh01  ap05  ap05  ap03   \n",
       "13  ap04  dh07  dh09  dh07  dh09  dh09  ap04  ap04  ap04  ap04  ap04  ap04   \n",
       "14  ap07  dh08  dh07  ap07  dh07  dh06  ap07  ap07  ap06  ap07  ap07  ap06   \n",
       "15  ap06  dh01  dh01  dh01  dh01  dh08  ap06  ap06  ap07  ap06  ap06  ap07   \n",
       "\n",
       "    dh07  dh08  dh09  dh10  dh11  \n",
       "0   dh01  dh06  dh06  dh03  dh10  \n",
       "1   dh09  dh11  dh07  dh06  dh08  \n",
       "2   dh02  dh10  dh10  dh04  dh06  \n",
       "3   dh04  dh09  dh04  dh11  dh03  \n",
       "4   dh06  dh03  dh03  dh09  dh04  \n",
       "5   dh05  dh04  dh08  ap01  ap01  \n",
       "6   dh10  ap01  dh02  dh08  dh09  \n",
       "7   dh03  dh07  dh01  dh05  ap05  \n",
       "8   ap01  dh05  dh11  dh02  dh05  \n",
       "9   dh08  ap05  dh05  dh07  dh07  \n",
       "10  dh11  dh02  ap01  ap05  dh02  \n",
       "11  ap03  dh01  ap05  dh01  ap03  \n",
       "12  ap05  ap03  ap03  ap03  dh01  \n",
       "13  ap04  ap04  ap04  ap04  ap04  \n",
       "14  ap07  ap06  ap06  ap06  ap06  \n",
       "15  ap06  ap07  ap07  ap07  ap07  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_map = pd.DataFrame(columns=sensors)\n",
    "for s in sensors:\n",
    "    dist_map[s] = df_dist.loc[~df_dist[s].isna(), s].sort_values().index\n",
    "print(dist_map.shape)\n",
    "dist_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(df, date, max_s=30):\n",
    "    '''\n",
    "    Need to have the distance map loaded into dist_map.\n",
    "    '''\n",
    "    # Copy df to keep track of previous NaNs\n",
    "    df_new = df.copy()\n",
    "    # Prepare dict to keep NaN fills\n",
    "    na_fixed = dict()\n",
    "    for col in df.columns:\n",
    "        na_fixed[col] = {'with_prev': 0, 'with_neigh': 0}\n",
    "    for row in df.index:\n",
    "        for col in df.columns:\n",
    "            if df.loc[row, col] == -99999:\n",
    "                ok = False\n",
    "                for gap in range(max_s):\n",
    "                    if row - gap < 0:\n",
    "                        break\n",
    "                    if df.loc[row - gap, col] != -99999:\n",
    "                        df_new.loc[row, col] = df.loc[row - gap, col]\n",
    "                        ok = True\n",
    "                        na_fixed[col]['with_prev'] += 1\n",
    "                        break\n",
    "                if not ok:\n",
    "                    for near_s in dist_map.loc[:, col]:\n",
    "                        if df.loc[row, near_s] != -99999:\n",
    "                            df_new.loc[row, col] = df.loc[row, near_s]\n",
    "                            ok = True\n",
    "                            na_fixed[col]['with_neigh'] += 1\n",
    "                            break\n",
    "                if not ok:\n",
    "                    print('Check out date {}: ({}, {})'.format(date, row, col))\n",
    "    return df_new.values, na_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_filling(in_path, out_path, max_rows=54001, n_cols=18):\n",
    "    # Parameters for the output hdf5 file\n",
    "    filters = tb.Filters(complib='blosc:lz4hc', complevel=9)\n",
    "    dtype = tb.Float32Atom()\n",
    "    data_shape = (0, n_cols)\n",
    "    chunkshape=(max_rows, n_cols)\n",
    "    tic = time.time()\n",
    "    # Open file and transfer\n",
    "    with tb.open_file(in_path, mode='r') as in_file, \\\n",
    "         tb.open_file(out_path, mode='a', filters=filters) as out_file, \\\n",
    "         warnings.catch_warnings() as w:\n",
    "        warnings.simplefilter('ignore', tb.NaturalNameWarning)\n",
    "        # Get sensor names\n",
    "        sensors = []\n",
    "        for sensor in in_file.get_node('/2010/03/18'):\n",
    "            sensors.append(sensor._v_name)\n",
    "        for year in in_file.get_node('/'):\n",
    "            for month in in_file.get_node(year):\n",
    "                for day in in_file.get_node(month):\n",
    "                    array = np.empty((max_rows, n_cols))\n",
    "                    for idx, sensor in enumerate(in_file.get_node(day)):\n",
    "                        array[:, idx + 1] = sensor[:, 1]\n",
    "                    # Add also timestamps\n",
    "                    array[:, 0] = sensor[:, 0]\n",
    "                    # Fill gaps\n",
    "                    array[:, 1:], na_fixed = fill_gaps(pd.DataFrame(array[:, 1:], columns=sensors), \n",
    "                                                       date=day._v_pathname)\n",
    "                    # Save to new hdf5 file\n",
    "                    node = out_file.create_carray(obj=array, createparents=True,\n",
    "                                                  where=month._v_pathname, name=day._v_name)\n",
    "                    # Add meta: column names and NaN fixed counts\n",
    "                    node._v_attrs['columns'] = str(['ts', *sensors])\n",
    "                    node._v_attrs['na_fixed'] = na_fixed\n",
    "    return 'It took {} in total'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 03:36:42 in total'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/raw/raw_carray.h5'\n",
    "out_path = '../data/filled_carray.h5'\n",
    "transfer_filling(in_path, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute mean and stdev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ap01</th>\n",
       "      <th>ap03</th>\n",
       "      <th>ap04</th>\n",
       "      <th>ap05</th>\n",
       "      <th>ap06</th>\n",
       "      <th>ap07</th>\n",
       "      <th>dh01</th>\n",
       "      <th>dh02</th>\n",
       "      <th>dh03</th>\n",
       "      <th>dh04</th>\n",
       "      <th>dh05</th>\n",
       "      <th>dh06</th>\n",
       "      <th>dh07</th>\n",
       "      <th>dh08</th>\n",
       "      <th>dh09</th>\n",
       "      <th>dh10</th>\n",
       "      <th>dh11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>369.583</td>\n",
       "      <td>371.677</td>\n",
       "      <td>370.767</td>\n",
       "      <td>374.43</td>\n",
       "      <td>376.328</td>\n",
       "      <td>375.62</td>\n",
       "      <td>370.251</td>\n",
       "      <td>371.368</td>\n",
       "      <td>377.211</td>\n",
       "      <td>370.59</td>\n",
       "      <td>374.523</td>\n",
       "      <td>375.172</td>\n",
       "      <td>376.577</td>\n",
       "      <td>371.386</td>\n",
       "      <td>375.343</td>\n",
       "      <td>374.901</td>\n",
       "      <td>371.611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stdev</th>\n",
       "      <td>347.86</td>\n",
       "      <td>351.347</td>\n",
       "      <td>348.776</td>\n",
       "      <td>355.133</td>\n",
       "      <td>355.936</td>\n",
       "      <td>357.222</td>\n",
       "      <td>349.585</td>\n",
       "      <td>351.749</td>\n",
       "      <td>363.1</td>\n",
       "      <td>350.484</td>\n",
       "      <td>354.856</td>\n",
       "      <td>354.726</td>\n",
       "      <td>356.311</td>\n",
       "      <td>351.574</td>\n",
       "      <td>363.053</td>\n",
       "      <td>353.144</td>\n",
       "      <td>351.126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ap01     ap03     ap04     ap05     ap06     ap07     dh01     dh02  \\\n",
       "mean   369.583  371.677  370.767   374.43  376.328   375.62  370.251  371.368   \n",
       "stdev   347.86  351.347  348.776  355.133  355.936  357.222  349.585  351.749   \n",
       "\n",
       "          dh03     dh04     dh05     dh06     dh07     dh08     dh09     dh10  \\\n",
       "mean   377.211   370.59  374.523  375.172  376.577  371.386  375.343  374.901   \n",
       "stdev    363.1  350.484  354.856  354.726  356.311  351.574  363.053  353.144   \n",
       "\n",
       "          dh11  \n",
       "mean   371.611  \n",
       "stdev  351.126  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_path = '../data/filled_01s.h5'\n",
    "day_length, n_days = 54001, 593\n",
    "\n",
    "with tb.open_file(out_path, mode='r') as out_file:\n",
    "    sensors = eval(out_file.get_node('/2010/03/18')._v_attrs['columns'])[1:]\n",
    "    df_stand = pd.DataFrame(index=['mean', 'stdev'], columns=sensors)\n",
    "    for s_id, sensor in enumerate(sensors):\n",
    "        sensor_s = np.empty((day_length * n_days,))\n",
    "        day_c = 0\n",
    "        for year in out_file.get_node('/'):\n",
    "            for month in out_file.get_node(year):\n",
    "                for day in out_file.get_node(month):\n",
    "                    sensor_s[day_length*day_c:day_length*(day_c+1)] = day[:, s_id + 1]\n",
    "                    day_c += 1\n",
    "        df_stand.loc['mean', sensor] = sensor_s.mean()\n",
    "        df_stand.loc['stdev', sensor] = sensor_s.std()\n",
    "display(df_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stand.to_csv('../data/other/mean_sd_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the record, a ts comprising all data for a single sensor (320M of records) takes about 256.18MB.\n"
     ]
    }
   ],
   "source": [
    "print('For the record, a ts comprising all data for a single sensor (320M of records) takes about {:0.5}MB.'.format(\n",
    "    sensor_s.nbytes / 10**6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_stand(in_path, out_path, df_stand, max_rows=54001, n_cols=18):\n",
    "    # Parameters for the output hdf5 file\n",
    "    filters = tb.Filters(complib='blosc:lz4hc', complevel=9)\n",
    "    dtype = tb.Float32Atom()\n",
    "    data_shape = (0, n_cols)\n",
    "    chunkshape=(max_rows, n_cols)\n",
    "    tic = time.time()\n",
    "    # Open file and transfer\n",
    "    with tb.open_file(in_path, mode='r') as in_file, \\\n",
    "         tb.open_file(out_path, mode='a', filters=filters) as out_file, \\\n",
    "         warnings.catch_warnings() as w:\n",
    "        warnings.simplefilter('ignore', tb.NaturalNameWarning)\n",
    "        # Get sensor names\n",
    "        sensors = eval(in_file.get_node('/2010/03/18')._v_attrs['columns'])[1:]\n",
    "        for year in in_file.get_node('/'):\n",
    "            for month in in_file.get_node(year):\n",
    "                for day in in_file.get_node(month):\n",
    "                    # Load data for that day\n",
    "                    array = day[:, :]\n",
    "                    # Standarize\n",
    "                    array[:, 1:] = array[:, 1:] - df_stand.loc['mean', :].values.reshape((1,n_cols-1))\n",
    "                    array[:, 1:] = array[:, 1:] / df_stand.loc['stdev', :].values.reshape((1,n_cols-1))\n",
    "                    # Save to new hdf5 file\n",
    "                    node = out_file.create_carray(obj=array, createparents=True,\n",
    "                                                  where=month._v_pathname, name=day._v_name)\n",
    "                    # Add meta: column names and NaN fixed counts\n",
    "                    node._v_attrs['columns'] = day._v_attrs['columns']\n",
    "                    node._v_attrs['na_fixed'] = day._v_attrs['na_fixed']\n",
    "    return 'It took {} in total'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:03:39 in total'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/filled_carray.h5'\n",
    "out_path = '../data/stand_carray.h5'\n",
    "transfer_stand(in_path, out_path, df_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample data to obtain coarser times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce new dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_resample(in_path, out_path, target_s, orig_rows=54001, n_cols=18):\n",
    "    target_rows = int((orig_rows - 1) / target_s)\n",
    "    # Parameters for the output hdf5 file\n",
    "    filters = tb.Filters(complib='blosc:lz4hc', complevel=9)\n",
    "    dtype = tb.Float32Atom()\n",
    "    data_shape = (0, n_cols)\n",
    "    chunkshape=(target_s, n_cols)\n",
    "    tic = time.time()\n",
    "    # Open file and transfer\n",
    "    with tb.open_file(in_path, mode='r') as in_file, \\\n",
    "         tb.open_file(out_path, mode='a', filters=filters) as out_file, \\\n",
    "         warnings.catch_warnings() as w:\n",
    "        warnings.simplefilter('ignore', tb.NaturalNameWarning)\n",
    "        # Get sensor names\n",
    "        sensors = eval(in_file.get_node('/2010/03/18')._v_attrs['columns'])[1:]\n",
    "        for year in in_file.get_node('/'):\n",
    "            for month in in_file.get_node(year):\n",
    "                for day in in_file.get_node(month):\n",
    "                    # Load data for that day\n",
    "                    array = day[:, :]\n",
    "                    # Downsample\n",
    "                    new_array = array[:-1, 1:].reshape(target_rows, target_s, len(sensors)).mean(axis=1)\n",
    "                    # Add new column\n",
    "                    new_array = np.c_[np.empty((target_rows, 1)), new_array]\n",
    "                    # Include time since epoch in new column\n",
    "                    new_array[:, 0] = array[:-1:target_s, 0]\n",
    "                    # Save to new hdf5 file\n",
    "                    node = out_file.create_carray(obj=new_array, createparents=True,\n",
    "                                                  where=month._v_pathname, name=day._v_name)\n",
    "                    # Add meta: column names and NaN fixed counts\n",
    "                    node._v_attrs['columns'] = day._v_attrs['columns']\n",
    "                    node._v_attrs['na_fixed'] = day._v_attrs['na_fixed']\n",
    "    return 'It took {} in total'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:15 in total'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/stand_carray.h5'\n",
    "out_path = '../data/stand_10s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:14 in total'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/filled_01s.h5'\n",
    "out_path = '../data/filled_10s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:11 in total'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/csi_haurwitz_01s.h5'\n",
    "out_path = '../data/csi_haurwitz_10s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:08 in total'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/stand_carray.h5'\n",
    "out_path = '../data/stand_30s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:08 in total'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/filled_01s.h5'\n",
    "out_path = '../data/filled_30s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:06 in total'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/csi_haurwitz_01s.h5'\n",
    "out_path = '../data/csi_haurwitz_30s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:07 in total'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/stand_carray.h5'\n",
    "out_path = '../data/stand_60s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:06 in total'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/filled_01s.h5'\n",
    "out_path = '../data/filled_60s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:05 in total'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/csi_haurwitz_01s.h5'\n",
    "out_path = '../data/csi_haurwitz_60s.h5'\n",
    "transfer_resample(in_path, out_path, target_s=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear-sky index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clear_sky(df_st, ts, model='ineichen'):\n",
    "    '''\n",
    "    Consider that ts has length batch_size + 1 (to account for the first ts).\n",
    "    '''\n",
    "    ts = pd.to_datetime(ts, unit='s').tz_localize('US/Hawaii')\n",
    "    sensors = df_st.index.values\n",
    "    # Compute clear-sky GHI for each sensor\n",
    "    cs = pd.DataFrame(columns=sensors)\n",
    "    for sensor in sensors:\n",
    "        aux = Location(*df_st.loc[sensor, ['Latitude', 'Longitude']].values)\n",
    "        # ineichen with climatology table by default\n",
    "        cs.loc[:, sensor] = aux.get_clearsky(ts, model).loc[:, 'ghi']\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_csi(in_path, out_path, model, max_csi=2.0, max_rows=54001, n_cols=18):\n",
    "    # Parameters for the output hdf5 file\n",
    "    filters = tb.Filters(complib='blosc:lz4hc', complevel=9)\n",
    "    dtype = tb.Float32Atom()\n",
    "    data_shape = (0, n_cols)\n",
    "    chunkshape=(max_rows, n_cols)\n",
    "    df_st = pd.read_csv('../data/other/stations.txt', '\\t', index_col=0)\n",
    "    tic = time.time()\n",
    "    # Open file and transfer\n",
    "    with tb.open_file(in_path, mode='r') as in_file, \\\n",
    "         tb.open_file(out_path, mode='a', filters=filters) as out_file, \\\n",
    "         warnings.catch_warnings() as w:\n",
    "        warnings.simplefilter('ignore', tb.NaturalNameWarning)\n",
    "        # Get sensor names\n",
    "        sensors = eval(in_file.get_node('/2010/03/18')._v_attrs['columns'])[1:]\n",
    "        for year in in_file.get_node('/'):\n",
    "            for month in in_file.get_node(year):\n",
    "                for day in in_file.get_node(month):\n",
    "                    # Load data for that day\n",
    "                    array = day[:, :]\n",
    "                    # Compute clear-sky index\n",
    "                    cs = compute_clear_sky(df_st, array[:, 0], model=model)\n",
    "                    array[:, 1:] = clearsky_index(array[:, 1:], cs.values, max_csi)\n",
    "                    # Save to new hdf5 file\n",
    "                    node = out_file.create_carray(obj=array, createparents=True,\n",
    "                                                  where=month._v_pathname, name=day._v_name)\n",
    "                    # Add meta: column names and NaN fixed counts\n",
    "                    node._v_attrs['columns'] = day._v_attrs['columns']\n",
    "                    node._v_attrs['na_fixed'] = day._v_attrs['na_fixed']\n",
    "    return 'It took {} in total'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iprado/py-3.7/lib/python3.7/site-packages/pvlib/irradiance.py:1155: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  clearsky_index = ghi / clearsky_ghi\n",
      "/home/iprado/py-3.7/lib/python3.7/site-packages/pvlib/irradiance.py:1155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  clearsky_index = ghi / clearsky_ghi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It took 02:43:13 in total'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['ineichen', 'haurwitz', 'simplified_solis']\n",
    "m_id = 2\n",
    "in_path = '../data/filled_01s.h5'\n",
    "out_path = '../data/csi_{}_01s.h5'.format(models[m_id])\n",
    "transfer_csi(in_path, out_path, models[m_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irradiance map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "def interp_map(arr, xy, X, Y):\n",
    "    '''\n",
    "    xy : Longitude and latitude of the available sensors.\n",
    "    X : x-coordinates of the mesh grid.\n",
    "    Y : y-coordinates of the mesh grid.\n",
    "    arr : Values from the sensors used to interpolate.\n",
    "    '''\n",
    "    return griddata(xy, arr, (X, Y), method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_map(in_path, out_path, map_dim=(54001, 10, 10)):\n",
    "    # Parameters for the output hdf5 file\n",
    "    filters = tb.Filters(complib='blosc:lz4hc', complevel=9)\n",
    "    dtype = tb.Float32Atom()\n",
    "    chunkshape = map_dim\n",
    "    df_st = pd.read_csv('../data/other/stations.txt', '\\t', index_col=0)\n",
    "    # Prepare coordinates for interpolation\n",
    "    xy = df_st.loc[:, ['Longitude', 'Latitude']].values\n",
    "    x = xy[:, 0]\n",
    "    y = xy[:, 1]\n",
    "    # For a 10 by 10 map\n",
    "    #xnew = np.linspace(min(x) - 0.001, max(x) + 0.001, 10)\n",
    "    #ynew = np.linspace(min(y) - 0.001, max(y) + 0.001, 10)\n",
    "    # For a 8 by 8 map\n",
    "    xnew = np.linspace(min(x) - 0.0006, max(x) + 0.0006, 8)\n",
    "    ynew = np.linspace(min(y) - 0.0006, max(y) + 0.0006, 8)\n",
    "    X, Y = np.meshgrid(xnew, ynew)\n",
    "    tic = time.time()\n",
    "    # Open file and transfer\n",
    "    with tb.open_file(in_path, mode='r') as in_file, \\\n",
    "         tb.open_file(out_path, mode='a', filters=filters) as out_file, \\\n",
    "         warnings.catch_warnings() as w:\n",
    "        warnings.simplefilter('ignore', tb.NaturalNameWarning)\n",
    "        # Get sensor names\n",
    "        sensors = eval(in_file.get_node('/2010/03/18')._v_attrs['columns'])[1:]\n",
    "        for year in in_file.get_node('/'):\n",
    "            for month in in_file.get_node(year):\n",
    "                for day in in_file.get_node(month):\n",
    "                    # Load data for that day\n",
    "                    array = day[:, :]\n",
    "                    # Interpolate gaps (watch the axes order!)\n",
    "                    map_arr = interp_map(array[:, 1:].T, xy, X, Y).transpose((2, 0, 1))\n",
    "                    # Save to new hdf5 file\n",
    "                    node = out_file.create_carray(obj=map_arr, createparents=True,\n",
    "                                                  where=month._v_pathname, name=day._v_name)\n",
    "                    # Add meta: column names\n",
    "                    node._v_attrs['columns'] = day._v_attrs['columns']\n",
    "    return 'It took {} in total'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:05:43 in total'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/stand_01s.h5'\n",
    "out_path = '../data/stand_map10_01s.h5'\n",
    "transfer_map(in_path, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:03:47 in total'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/stand_01s.h5'\n",
    "out_path = '../data/stand_map08_01s.h5'\n",
    "transfer_map(in_path, out_path, map_dim=(54001, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample data to obtain coarser times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_resample(in_path, out_path, target_s, orig_dim=(54001, 10, 10)):\n",
    "    target_t = int((orig_dim[0] - 1) / target_s)\n",
    "    # Parameters for the output hdf5 file\n",
    "    filters = tb.Filters(complib='blosc:lz4hc', complevel=9)\n",
    "    dtype = tb.Float32Atom()\n",
    "    chunkshape = (target_t, *orig_dim[1:])\n",
    "    tic = time.time()\n",
    "    # Open file and transfer\n",
    "    with tb.open_file(in_path, mode='r') as in_file, \\\n",
    "         tb.open_file(out_path, mode='a', filters=filters) as out_file, \\\n",
    "         warnings.catch_warnings() as w:\n",
    "        warnings.simplefilter('ignore', tb.NaturalNameWarning)\n",
    "        for year in in_file.get_node('/'):\n",
    "            for month in in_file.get_node(year):\n",
    "                for day in in_file.get_node(month):\n",
    "                    # Load data for that day\n",
    "                    array = day[:, :]\n",
    "                    # Downsample\n",
    "                    new_array = array[:-1, :, :].reshape((target_t, target_s, *orig_dim[1:])).mean(axis=1)\n",
    "                    # Save to new hdf5 file\n",
    "                    node = out_file.create_carray(obj=new_array, createparents=True,\n",
    "                                                  where=month._v_pathname, name=day._v_name)\n",
    "                    # Add meta: column names\n",
    "                    node._v_attrs['columns'] = day._v_attrs['columns']\n",
    "    return 'It took {} in total'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:38 in total'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/stand_map10_01s.h5'\n",
    "out_path = '../data/stand_map10_01m.h5'\n",
    "transfer_resample(in_path, out_path, target_s=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It took 00:00:22 in total'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = '../data/stand_map08_01s.h5'\n",
    "out_path = '../data/stand_map08_01m.h5'\n",
    "transfer_resample(in_path, out_path, target_s=60, orig_dim=(54001, 8, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "326px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
